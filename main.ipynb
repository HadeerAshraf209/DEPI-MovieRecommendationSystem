{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1B3zzWCkWp3B28i7bIuht5ZVQd7aLYFlk","timestamp":1728758116359}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b-1huWTWY0nT","executionInfo":{"status":"ok","timestamp":1728747699028,"user_tz":-180,"elapsed":26590,"user":{"displayName":"Hanin Eltabakh","userId":"00111303788456335605"}},"outputId":"71eb36de-bd21-4554-8aea-6cda31e3d51a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting azure-storage-blob\n","  Downloading azure_storage_blob-12.23.1-py3-none-any.whl.metadata (26 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n","Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (3.4.1)\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (1.4.2)\n","Collecting gradio\n","  Downloading gradio-5.0.2-py3-none-any.whl.metadata (15 kB)\n","Collecting azure-core>=1.30.0 (from azure-storage-blob)\n","  Downloading azure_core-1.31.0-py3-none-any.whl.metadata (39 kB)\n","Requirement already satisfied: cryptography>=2.1.4 in /usr/local/lib/python3.10/dist-packages (from azure-storage-blob) (43.0.1)\n","Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from azure-storage-blob) (4.12.2)\n","Collecting isodate>=0.6.1 (from azure-storage-blob)\n","  Downloading isodate-0.7.2-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\n","Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.9.2)\n","Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras) (0.0.8)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.11.0)\n","Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras) (0.13.0)\n","Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.4.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras) (24.1)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n","Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n","Collecting aiofiles<24.0,>=22.0 (from gradio)\n","  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n","Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n","Collecting fastapi<1.0 (from gradio)\n","  Downloading fastapi-0.115.2-py3-none-any.whl.metadata (27 kB)\n","Collecting ffmpy (from gradio)\n","  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n","Collecting gradio-client==1.4.0 (from gradio)\n","  Downloading gradio_client-1.4.0-py3-none-any.whl.metadata (7.1 kB)\n","Collecting httpx>=0.24.1 (from gradio)\n","  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n","Collecting huggingface-hub>=0.25.1 (from gradio)\n","  Downloading huggingface_hub-0.25.2-py3-none-any.whl.metadata (13 kB)\n","Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n","Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n","Collecting orjson~=3.0 (from gradio)\n","  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m396.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (10.4.0)\n","Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.9.2)\n","Collecting pydub (from gradio)\n","  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n","Collecting python-multipart>=0.0.9 (from gradio)\n","  Downloading python_multipart-0.0.12-py3-none-any.whl.metadata (1.9 kB)\n","Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n","Collecting ruff>=0.2.2 (from gradio)\n","  Downloading ruff-0.6.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n","Collecting semantic-version~=2.0 (from gradio)\n","  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n","Collecting tomlkit==0.12.0 (from gradio)\n","  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n","Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.5)\n","Collecting uvicorn>=0.14.0 (from gradio)\n","  Downloading uvicorn-0.31.1-py3-none-any.whl.metadata (6.6 kB)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.4.0->gradio) (2024.6.1)\n","Collecting websockets<13.0,>=10.0 (from gradio-client==1.4.0->gradio)\n","  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=2.1.4->azure-storage-blob) (1.17.1)\n","Collecting starlette<0.41.0,>=0.37.2 (from fastapi<1.0->gradio)\n","  Downloading starlette-0.39.2-py3-none-any.whl.metadata (6.0 kB)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n","Collecting httpcore==1.* (from httpx>=0.24.1->gradio)\n","  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n","Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.24.1->gradio)\n","  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (3.16.1)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (4.66.5)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.23.4)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (2.18.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob) (2.22)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n","Downloading azure_storage_blob-12.23.1-py3-none-any.whl (405 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m405.6/405.6 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading gradio-5.0.2-py3-none-any.whl (42.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading gradio_client-1.4.0-py3-none-any.whl (319 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.8/319.8 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n","Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n","Downloading azure_core-1.31.0-py3-none-any.whl (197 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m197.4/197.4 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fastapi-0.115.2-py3-none-any.whl (94 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.7/94.7 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading huggingface_hub-0.25.2-py3-none-any.whl (436 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m436.6/436.6 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading isodate-0.7.2-py3-none-any.whl (22 kB)\n","Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_multipart-0.0.12-py3-none-any.whl (23 kB)\n","Downloading ruff-0.6.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n","Downloading uvicorn-0.31.1-py3-none-any.whl (63 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n","Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n","Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading starlette-0.39.2-py3-none-any.whl (73 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pydub, websockets, tomlkit, semantic-version, ruff, python-multipart, orjson, isodate, h11, ffmpy, aiofiles, uvicorn, starlette, huggingface-hub, httpcore, azure-core, httpx, fastapi, azure-storage-blob, gradio-client, gradio\n","  Attempting uninstall: huggingface-hub\n","    Found existing installation: huggingface-hub 0.24.7\n","    Uninstalling huggingface-hub-0.24.7:\n","      Successfully uninstalled huggingface-hub-0.24.7\n","Successfully installed aiofiles-23.2.1 azure-core-1.31.0 azure-storage-blob-12.23.1 fastapi-0.115.2 ffmpy-0.4.0 gradio-5.0.2 gradio-client-1.4.0 h11-0.14.0 httpcore-1.0.6 httpx-0.27.2 huggingface-hub-0.25.2 isodate-0.7.2 orjson-3.10.7 pydub-0.25.1 python-multipart-0.0.12 ruff-0.6.9 semantic-version-2.10.0 starlette-0.39.2 tomlkit-0.12.0 uvicorn-0.31.1 websockets-12.0\n"]}],"source":["!pip install azure-storage-blob pandas numpy keras tensorflow joblib gradio\n"]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from datetime import datetime\n","from azure.storage.blob import BlobServiceClient\n","from io import StringIO\n","from keras.models import Model, load_model\n","from keras.layers import Input, Embedding, Flatten, Dense, Concatenate, Dropout, BatchNormalization\n","from keras.optimizers import AdamW\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","import joblib\n","import gradio as gr\n"],"metadata":{"id":"--E-N81fY6E-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["connection_string = (\n","    \"DefaultEndpointsProtocol=https;AccountName=netflixrecommendation;AccountKey=KTWSAJ2ds/jnkpU9PmkX6U28mLrVEtP2yZ7+nKDcF4EzPgPpuu+uJB54VEYwq8gp+N8J55kgBpxl+AStA61cHg==;EndpointSuffix=core.windows.net\"\n",")\n","\n","blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n","container_name = \"netflix-data\"\n","container_client = blob_service_client.get_container_client(container_name)\n"],"metadata":{"id":"Ea9FwTHxZDS-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load_csv_from_blob(blob_name):\n","    blob_client = container_client.get_blob_client(blob_name)\n","    blob_data = blob_client.download_blob().readall()\n","    csv_str = blob_data.decode('utf-8')\n","    data = StringIO(csv_str)\n","    return pd.read_csv(data)\n","\n","movies_data = load_csv_from_blob(\"movies.csv\")\n","ratings_data = load_csv_from_blob(\"ratings.csv\")\n"],"metadata":{"id":"j0Mi31LLZDWy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert userId and movieId to categorical codes for model compatibility\n","ratings_data['userId'] = ratings_data['userId'].astype('category').cat.codes\n","ratings_data['movieId'] = ratings_data['movieId'].astype('category').cat.codes\n","\n","num_users = ratings_data['userId'].nunique()\n","num_movies = ratings_data['movieId'].nunique()\n","\n","# Split the data into training and testing sets\n","train_data, test_data = train_test_split(ratings_data, test_size=0.2, random_state=42)\n"],"metadata":{"id":"bm37nd4oZDai"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_ncf_model(num_users, num_movies, embedding_dim=50):\n","    user_input = Input(shape=(1,), name='user_input')\n","    movie_input = Input(shape=(1,), name='movie_input')\n","\n","    user_embedding = Embedding(num_users, embedding_dim, name='user_embedding')(user_input)\n","    movie_embedding = Embedding(num_movies, embedding_dim, name='movie_embedding')(movie_input)\n","\n","    user_vector = Flatten()(user_embedding)\n","    movie_vector = Flatten()(movie_embedding)\n","\n","    concatenated = Concatenate()([user_vector, movie_vector])\n","    hidden_layer = Dense(128, activation='relu')(concatenated)\n","    hidden_layer = BatchNormalization()(hidden_layer)\n","    hidden_layer = Dropout(0.2)(hidden_layer)  # Regularization\n","\n","    hidden_layer = Dense(64, activation='relu')(hidden_layer)\n","    output_layer = Dense(1)(hidden_layer)\n","\n","    model = Model(inputs=[user_input, movie_input], outputs=output_layer)\n","    model.compile(optimizer=AdamW(learning_rate=0.001), loss='mean_squared_error')\n","\n","    return model\n"],"metadata":{"id":"O_cAtAGCZDef"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_model(train_data, num_users, num_movies):\n","    model = create_ncf_model(num_users, num_movies)\n","    X_train = [train_data['userId'].values, train_data['movieId'].values]\n","    y_train = train_data['rating'].values\n","\n","    model.fit(\n","        X_train, y_train, epochs=10, batch_size=256, verbose=1,\n","        validation_split=0.1\n","    )\n","\n","    model.save(\"ncf_model.h5\")\n","    return model\n","\n","model = train_model(train_data, num_users, num_movies)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2Ayt9YT0ZDiY","executionInfo":{"status":"ok","timestamp":1728748709028,"user_tz":-180,"elapsed":32647,"user":{"displayName":"Hanin Eltabakh","userId":"00111303788456335605"}},"outputId":"53cfcd91-9968-4e6d-d951-f90ec76ca252"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 4.5915 - val_loss: 6.8658\n","Epoch 2/10\n","\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.8245 - val_loss: 2.1990\n","Epoch 3/10\n","\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6974 - val_loss: 0.8853\n","Epoch 4/10\n","\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 0.6040 - val_loss: 0.8705\n","Epoch 5/10\n","\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.5469 - val_loss: 0.8838\n","Epoch 6/10\n","\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.5042 - val_loss: 0.9008\n","Epoch 7/10\n","\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.4582 - val_loss: 0.9191\n","Epoch 8/10\n","\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.4297 - val_loss: 0.9282\n","Epoch 9/10\n","\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.4056 - val_loss: 0.9345\n","Epoch 10/10\n","\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.3824 - val_loss: 0.9424\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]}]},{"cell_type":"code","source":["def load_trained_model():\n","    return load_model(\"ncf_model.h5\")\n","\n","def recommend_movies(user_id, n_recommendations=5):\n","    model = load_trained_model()\n","\n","    # Filter out movies the user has already rated\n","    user_ratings = ratings_data[ratings_data['userId'] == user_id]\n","    rated_movie_ids = user_ratings['movieId'].unique()\n","\n","    all_movie_ids = ratings_data['movieId'].unique()\n","    not_watched = list(set(all_movie_ids) - set(rated_movie_ids))\n","\n","    if not_watched:\n","        movie_ids = np.array(not_watched)\n","        user_array = np.full_like(movie_ids, user_id)\n","\n","        # Predict ratings for unwatched movies\n","        predicted_ratings = model.predict([user_array, movie_ids]).flatten()\n","\n","        recommendations = pd.DataFrame({\n","            'movieId': movie_ids,\n","            'predicted_rating': predicted_ratings\n","        }).merge(movies_data, on='movieId', how='left')\n","\n","        return recommendations.sort_values('predicted_rating', ascending=False).head(n_recommendations)\n","    else:\n","        return None\n","\n"],"metadata":{"id":"jzTtYZgxZDmC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"mCo0SmrsaawK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def gradio_recommend(user_id):\n","    try:\n","        user_id = int(user_id)\n","        recommendations = recommend_movies(user_id)\n","\n","        if recommendations is not None:\n","            result = \"\\n\".join(\n","                f\"{row['title']} (Predicted Rating: {row['predicted_rating']:.2f})\"\n","                for _, row in recommendations.iterrows()\n","            )\n","            return result\n","        else:\n","            return f\"No recommendations available for user {user_id}.\"\n","    except ValueError:\n","        return \"Please enter a valid numeric user ID.\"\n","\n","iface = gr.Interface(\n","    fn=gradio_recommend,\n","    inputs=gr.Textbox(label=\"Enter User ID\"),\n","    outputs=gr.Textbox(label=\"Recommendations\"),\n","    title=\"Movie Recommendation System\",\n","    description=\"Get movie recommendations based on predicted ratings.\"\n",")\n"],"metadata":{"id":"VnK45lAPZDpk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n"],"metadata":{"id":"-Nbl8bwdae1S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["iface.launch()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":646},"id":"HageEuiYZDtc","executionInfo":{"status":"ok","timestamp":1728748874541,"user_tz":-180,"elapsed":1080,"user":{"displayName":"Hanin Eltabakh","userId":"00111303788456335605"}},"outputId":"5b244071-cb6d-44f5-b43a-77a146d6059b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n","\n","Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","* Running on public URL: https://225edd68deca57bd0b.gradio.live\n","\n","This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://225edd68deca57bd0b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":39}]},{"cell_type":"code","source":[],"metadata":{"id":"BexjCz1qZDxp"},"execution_count":null,"outputs":[]}]}